{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP Experiments to verify theory for L-0 norm - CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys, os, gc, math\n",
    "import numpy as np\n",
    "from scipy.fftpack import dct,idct\n",
    "from keras.datasets import mnist, fashion_mnist,cifar10\n",
    "from PIL import Image\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from models.util import *\n",
    "\n",
    "\n",
    "#Seed used for choosing classes, training points, and test points.\n",
    "#SEED = 14\n",
    "SEED=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 500\n",
    "sqrt_n = 32\n",
    "input_shape=(sqrt_n,sqrt_n,1)\n",
    "n = sqrt_n*sqrt_n\n",
    "k = 10\n",
    "c=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four(k,n,c):\n",
    "    a = ((n/c) - (k**2))**2\n",
    "    b = (c/(n*k))\n",
    "    return a*(1/4.0)*b\n",
    "\n",
    "def book(k,n,c):\n",
    "    return (np.sqrt(n/(c*41))*4 - 2*k  )*0.5 \n",
    "\n",
    "def rns(k,t,n,c):\n",
    "    return ( np.sqrt(k+t)*(1.0/np.sqrt(k) + 1.0/np.sqrt(t))*np.sqrt((4*c*k*t)/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load Cifar10 data\n",
    "(X_train, _), (X_test, _) = cifar10.load_data()\n",
    "m_data = np.concatenate((X_train,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "m_data = m_data/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise budget of 8\n"
     ]
    }
   ],
   "source": [
    "#Check Cifar10 results for 500 random images - BP\n",
    "t = int(four(float(k),float(n),float(c)))\n",
    "print('Noise budget of %d' % t)\n",
    "subset_idx = np.random.choice(np.arange(m_data.shape[0]),num_samples)\n",
    "m_data_sub_bp = m_data[subset_idx]\n",
    "m_data_y_bp = np.zeros((num_samples,sqrt_n,sqrt_n,3))\n",
    "t_values_bp = np.zeros((num_samples,3))\n",
    "for i in range(num_samples):\n",
    "    #first sample an element from the data\n",
    "    x_r = m_data_sub_bp[i,:,:,0].flatten()\n",
    "    x_g = m_data_sub_bp[i,:,:,1].flatten()\n",
    "    x_b = m_data_sub_bp[i,:,:,2].flatten()\n",
    "\n",
    "    #Now sample a t - must be atleast 1\n",
    "    t_r = np.random.randint(1,t)\n",
    "    t_g = np.random.randint(1,t)\n",
    "    t_b = np.random.randint(1,t)\n",
    "    t_values_bp[i,0] = t_r\n",
    "    t_values_bp[i,1] = t_g\n",
    "    t_values_bp[i,2] = t_b\n",
    "\n",
    "\n",
    "    #Now samnple the an index set\n",
    "    s_r = np.random.choice(np.arange(n),t_r)\n",
    "    s_g = np.random.choice(np.arange(n),t_g)\n",
    "    s_b = np.random.choice(np.arange(n),t_b)\n",
    "\n",
    "\n",
    "    e = np.zeros((n,3))\n",
    "    #Now create the vector e\n",
    "    #pick a value for each element between 0 and 1 as the images are normalized\n",
    "    for j in range(t_r):\n",
    "        e[s_r[j],0] = np.random.uniform()\n",
    "    for j in range(t_g):\n",
    "        e[s_g[j],1] = np.random.uniform()\n",
    "    for j in range(t_b):\n",
    "        e[s_b[j],2] = np.random.uniform()\n",
    "    y = m_data_sub_bp[i,:,:,:] + e.reshape((sqrt_n,sqrt_n,3))\n",
    "    m_data_y_bp[i,:,:,:] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get \\delta_p and \\Delta_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Form the matrix F\n",
    "F = get_matrix(n,tf='dct')\n",
    "I = np.identity(n)\n",
    "A = np.concatenate((F.T,I), axis=1)\n",
    "n = 32*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate(z):\n",
    "    y,eta = z\n",
    "    x_hat = socp(y,A,n=2*n,eta=eta)[:n]\n",
    "    return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture three\n",
    "errors_l2_bp = np.zeros(m_data_y_bp.shape[0])\n",
    "bot_l2_bp = np.zeros((m_data_y_bp.shape[0],3))\n",
    "diff_l2_bp = np.zeros(m_data_y_bp.shape[0])\n",
    "\n",
    "for i in range(num_samples):\n",
    "    y_r = m_data_y_bp[i,:,:,0].flatten()\n",
    "    x_r = m_data_sub_bp[i,:,:,0].flatten()\n",
    "    \n",
    "    #Get actual top k and bottom k (we use the faster transform which may introduce some error)\n",
    "    x_hat_top_k_r, x_hat_bot_k_r =  get_top_bot_k_vec(dct(x_r, norm='ortho'),k=k)    \n",
    "    e_r = y_r - x_r \n",
    "    eta_r = bot_l2_bp[i,0] = np.linalg.norm(x_hat_bot_k_r)\n",
    "    \n",
    "    y_g = m_data_y_bp[i,:,:,1].flatten()\n",
    "    x_g = m_data_sub_bp[i,:,:,1].flatten()\n",
    "    \n",
    "    #Get actual top k and bottom k (we use the faster transform which may introduce some error)\n",
    "    x_hat_top_k_g, x_hat_bot_k_g =  get_top_bot_k_vec(dct(x_g, norm='ortho'),k=k)    \n",
    "    e_g = y_g - x_g\n",
    "    eta_g = bot_l2_bp[i,1] = np.linalg.norm(x_hat_bot_k_g)\n",
    "    \n",
    "    \n",
    "    y_b = m_data_y_bp[i,:,:,2].flatten()\n",
    "    x_b= m_data_sub_bp[i,:,:,2].flatten()\n",
    "    \n",
    "    #Get actual top k and bottom k (we use the faster transform which may introduce some error)\n",
    "    x_hat_top_k_b, x_hat_bot_k_b =  get_top_bot_k_vec(dct(x_b, norm='ortho'),k=k)    \n",
    "    e_b = y_b- x_b\n",
    "    eta_b = bot_l2_bp[i,2] = np.linalg.norm(x_hat_bot_k_b)\n",
    "\n",
    "\n",
    "    args = [(y_r,eta_r), (y_g,eta_g), (y_b,eta_b)]\n",
    "    p = Pool(3)\n",
    "    approximates = p.map(approximate, args)\n",
    "    p.terminate()\n",
    "\n",
    "    x_hat_approx_r = approximates[0]\n",
    "    x_hat_approx_g = approximates[1]\n",
    "    x_hat_approx_b = approximates[2]\n",
    "    \n",
    "    \n",
    "    delta_r = np.sqrt( (c*float(k)*t_values_bp[i,0])/float(n))\n",
    "    mu_r = np.sqrt(1 + delta_r)/(1-delta_r)\n",
    "    theta_r = (np.sqrt(k+t_values_bp[i,0])/(1 - delta_r))*np.sqrt((float(k)*c)/float(n))\n",
    "    beta = np.sqrt((float(k)*c)/float(n))\n",
    "    upper_r = ( (2*mu_r*np.sqrt(k+t_values_bp[i,0]))/(1-theta_r) )*(1 + (beta/(1-delta_r)) )*eta_r\n",
    "    \n",
    "        \n",
    "    delta_g = np.sqrt( (c*float(k)*t_values_bp[i,1])/float(n))\n",
    "    mu_g = np.sqrt(1 + delta_g)/(1-delta_g)\n",
    "    theta_g = (np.sqrt(k+t_values_bp[i,1])/(1 - delta_g))*np.sqrt((float(k)*c)/float(n))\n",
    "    upper_g = ( (2*mu_g*np.sqrt(k+t_values_bp[i,1]))/(1-theta_g) )*(1 + (beta/(1-delta_g)) )*eta_g\n",
    "    \n",
    "        \n",
    "    delta_b = np.sqrt( (c*float(k)*t_values_bp[i,2])/float(n))\n",
    "    mu_b = np.sqrt(1 + delta_b)/(1-delta_b)\n",
    "    theta_b = (np.sqrt(k+t_values_bp[i,2])/(1 - delta_b))*np.sqrt((float(k)*c)/float(n))\n",
    "    upper_b = ( (2*mu_b*np.sqrt(k+t_values_bp[i,2]))/(1-theta_b) )*(1 + (beta/(1-delta_b)) )*eta_b\n",
    "    \n",
    "    #Note the errors\n",
    "    errors_l2_bp[i] = np.linalg.norm(x_hat_top_k_r.flatten()- x_hat_approx_r.flatten()) + \\\n",
    "                        np.linalg.norm(x_hat_top_k_g.flatten()- x_hat_approx_g.flatten()) + \\\n",
    "                         np.linalg.norm(x_hat_top_k_b.flatten()- x_hat_approx_b.flatten())\n",
    "   \n",
    "    #Calculate the difference from the upper bound\n",
    "    diff_l2_bp[i] = (upper_r + upper_g + upper_b) - errors_l2_bp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.032666666666667 20.105961380672948 866.0788459597803\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(t_values_bp), \n",
    "      np.mean(errors_l2_bp), \n",
    "      np.mean(diff_l2_bp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_tup_bp = (m_data_y_bp, m_data_sub_bp, t_values_bp,errors_l2_bp, diff_l2_bp )\n",
    "\n",
    "import pickle\n",
    "with open('data/cifar10_theory_socp_l0.pickle', 'wb') as f:\n",
    "    pickle.dump(mnist_tup_bp, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
