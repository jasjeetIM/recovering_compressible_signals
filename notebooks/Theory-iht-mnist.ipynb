{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys, os, gc, math\n",
    "import numpy as np\n",
    "from scipy.fftpack import dct,idct\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from models.util import *\n",
    "\n",
    "\n",
    "#Seed used for choosing classes, training points, and test points.\n",
    "#SEED = 14\n",
    "SEED=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 500\n",
    "sqrt_n = 125\n",
    "input_shape=(sqrt_n,sqrt_n,1)\n",
    "n = sqrt_n*sqrt_n\n",
    "k = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load MNIST data\n",
    "(X_train, _), (X_test, _) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "m_data_small = np.concatenate((X_train,X_test))\n",
    "            \n",
    "            \n",
    "m_data = np.zeros((m_data_small.shape[0],sqrt_n,sqrt_n,1))\n",
    "            \n",
    "for i in range(m_data.shape[0]):\n",
    "    img = m_data_small[i,:,:,0]\n",
    "    img = Image.fromarray(img)\n",
    "    basewidth = sqrt_n\n",
    "    wpercent = (basewidth/float(img.size[0]))\n",
    "    hsize = int((float(img.size[1])*float(wpercent)))\n",
    "    img = img.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "    m_data[i,:,:,0] = np.asarray(img)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "m_data = m_data/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check MNIST results for 1000 random images\n",
    "#n=40000, k=40, t <= (n/27*k) ~ 37.03\n",
    "c=2.0\n",
    "t = int(n/27/k/c)\n",
    "subset_idx = np.random.choice(np.arange(m_data.shape[0]),num_samples)\n",
    "m_data_sub = m_data[subset_idx]\n",
    "m_data_y = np.zeros((num_samples,sqrt_n,sqrt_n))\n",
    "t_values = np.zeros(num_samples)\n",
    "for i in range(num_samples):\n",
    "    #first sample an element from the data\n",
    "    x = m_data_sub[i,:,:,0].flatten()\n",
    "    #Now sample a t - must be atleast 1\n",
    "    t_l = np.random.randint(1,t)\n",
    "    t_values[i] = t_l\n",
    "    #Now samnple the an index set from [n] with cardinality = t_l\n",
    "    s = np.random.choice(np.arange(n),t_l)\n",
    "    e = np.zeros(n)\n",
    "    #Now create the vector e\n",
    "    #pick a value for each element between 0 and 1 as the images are normalized\n",
    "    for j in range(t_l):\n",
    "        e[s[j]] = np.random.uniform()\n",
    "    y = x + e\n",
    "    m_data_y[i,:,:] = y.reshape((sqrt_n,sqrt_n))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture one\n",
    "#MNIST IHT\n",
    "errors_l2 = np.zeros(m_data_y.shape[0])\n",
    "errors_inf = np.zeros(m_data_y.shape[0])\n",
    "bot_l2 = np.zeros(m_data_y.shape[0])\n",
    "tau = np.zeros(m_data_y.shape[0])\n",
    "diff_l2 = np.zeros(m_data_y.shape[0])\n",
    "diff_inf = np.zeros(m_data_y.shape[0])\n",
    "T_values = np.zeros(m_data_y.shape[0])\n",
    "\n",
    "for i in range(num_samples):\n",
    "    y = m_data_y[i,:,:].flatten()\n",
    "    x = m_data_sub[i,:,:].flatten()\n",
    "    \n",
    "    #Get actual top k and bottom k\n",
    "    x_hat_top_k, x_hat_bot_k =  get_top_bot_k_vec(dct(x, norm='ortho'),k=k)\n",
    "    #Now we need to calculate T\n",
    "    e = y - x \n",
    "    #Get the constant that we need to set to zero as a function of T\n",
    "    a = np.sqrt(np.linalg.norm(x_hat_top_k)**2 + np.linalg.norm(e)**2)\n",
    "    #Lets get rho\n",
    "    rho = np.sqrt(27)*np.sqrt(c*k*t_values[i]/float(n))\n",
    "    \n",
    "    #We want to find a T such that rho^T a is very small - we use 1e-20 \n",
    "    T = int((np.log(1e-20) - np.log(a))/np.log(rho))\n",
    "    T_values[i] = T\n",
    "    #Get top k approx\n",
    "    x_hat_approx,e_hat_approx = iht(y,int(t_values[i]),T=T,k=k)\n",
    "    \n",
    "    #Note the errors\n",
    "    errors_l2[i] = np.linalg.norm(x_hat_top_k.flatten()- x_hat_approx.flatten())\n",
    "    errors_inf[i] = np.linalg.norm(x_hat_top_k.flatten()- x_hat_approx.flatten(), ord=np.inf)\n",
    "\n",
    "    #Note the norm of bottom k coefficients\n",
    "    bot_l2[i] = np.linalg.norm(x_hat_bot_k)\n",
    "\n",
    "    #Get the multiplicative constant\n",
    "    c_l2 = np.sqrt(4*c*k*t_values[i]/n)\n",
    "    c_inf = np.sqrt(2*c*t_values[i]/n)\n",
    "    #Calculate tau\n",
    "    tau[i] = (np.sqrt(3)*np.sqrt(1 + 2*np.sqrt(c*k*t_values[i]/n)))/(1-rho)\n",
    "    \n",
    "    #Calculate the difference from the upper bound\n",
    "    diff_l2[i] = (c_l2*tau[i]*bot_l2[i]) - errors_l2[i] \n",
    "    diff_inf[i] = (c_inf*tau[i]*bot_l2[i]) - errors_inf[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.486 12.880381823912398 0.6865346126117067 128.30272947652045 0.44548317008755417 21.675993119772038\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(t_values), \n",
    "      np.mean(tau), \n",
    "      np.mean(errors_l2), \n",
    "      np.mean(diff_l2), \n",
    "      np.mean(errors_inf), \n",
    "      np.mean(diff_inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_tup = (m_data_y, m_data_sub, t_values,tau,errors_l2, errors_inf, diff_l2, diff_inf )\n",
    "\n",
    "import pickle\n",
    "with open('data/mnist_tuple_theory.pickle', 'wb') as f:\n",
    "    pickle.dump(mnist_tup, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
